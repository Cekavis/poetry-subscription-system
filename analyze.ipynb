{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化 poems & words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "\n",
    "poems = {}\n",
    "with open('poems.txt', 'r') as f:\n",
    "    poem = {'content': []}\n",
    "    for l in f.readlines():\n",
    "        l = l.split(' ')\n",
    "        line = re.sub(r'\\([^)]*\\)', '', ''.join(l[3:]))\n",
    "        line = line.strip()\n",
    "        if l[2]=='-100':\n",
    "            if poem['content']:\n",
    "                poems[poem['author']] = poems.get(poem['author'], [])\n",
    "                poems[poem['author']].append(poem)\n",
    "            poem = {'title': line, 'content': []}\n",
    "        elif l[2]=='-10':\n",
    "            assert 'title' in poem\n",
    "            poem['note'] = line\n",
    "        elif l[2]=='-1':\n",
    "            assert 'title' in poem\n",
    "            poem['author'] = line\n",
    "        elif int(l[2])>0:\n",
    "            assert 'title' in poem\n",
    "            poem['content'].append(line)\n",
    "            assert len(poem['content']) == int(l[2])\n",
    "    if poem['content']:\n",
    "        poems[poem['author']] = poems.get(poem['author'], [])\n",
    "        poems[poem['author']].append(poem)\n",
    "\n",
    "poems['all'] = [x for y in poems.values() for x in y]\n",
    "\n",
    "words = []\n",
    "with open('wordlist.txt', 'r') as f:\n",
    "    words = [w.strip() for w in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算 idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = {}\n",
    "cnt = {word: 0 for word in words}\n",
    "\n",
    "for poem in poems['all']:\n",
    "    s = set()\n",
    "    for l in poem['content']:\n",
    "        for i in range(1, 4):\n",
    "            for j in range(len(l)-i):\n",
    "                s.add(l[j:j+i])\n",
    "    for word in s:\n",
    "        if word in cnt:\n",
    "            cnt[word] += 1\n",
    "                \n",
    "for word in cnt:\n",
    "    if cnt[word]>10:\n",
    "        idf[word] = math.log10(len(poems['all'])/(1+cnt[word]))\n",
    "\n",
    "idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算 tf-idf 向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_w = np.zeros((len(idf), len(poems['all'])), dtype=np.float64)\n",
    "tfidf_p = np.zeros((len(poems['all']), len(idf)), dtype=np.float64)\n",
    "idx_w = {}\n",
    "for idx, word in enumerate(idf):\n",
    "    idx_w[word] = idx\n",
    "for idx_p, poem in enumerate(poems['all']):\n",
    "    for l in poem['content']:\n",
    "        cnt = 0\n",
    "        for i in range(1, 4):\n",
    "            for j in range(len(l)-i):\n",
    "                word = l[j:j+i]\n",
    "                if word in idf:\n",
    "                    cnt += 1\n",
    "        for i in range(1, 4):\n",
    "            for j in range(len(l)-i):\n",
    "                word = l[j:j+i]\n",
    "                if word in idf:\n",
    "                    tfidf_w[idx_w[word]][idx_p] += 1/cnt*idf[word]\n",
    "                    tfidf_p[idx_p][idx_w[word]] += 1/cnt*idf[word]\n",
    "for v in tfidf_w:\n",
    "    v /= np.linalg.norm(v)\n",
    "for v in tfidf_p:\n",
    "    v /= np.linalg.norm(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对词向量和诗向量降维（PCA）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "model_w = PCA(n_components=500)\n",
    "model_w.fit(tfidf_w)\n",
    "tfidf_w = model_w.transform(tfidf_w)\n",
    "\n",
    "model_p = PCA(n_components=500)\n",
    "model_p.fit(tfidf_p)\n",
    "tfidf_p = model_p.transform(tfidf_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输出 tf-idf 信息到文本文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tf-idf.txt', 'w') as f:\n",
    "    for i in idx_w:\n",
    "        f.write(i + '：' + str(tfidf_w[idx_w[i]]) + '\\n')\n",
    "    for idx_p, p in enumerate(poems['all']):\n",
    "        f.write(p['title'] + '：' + str(tfidf_p[idx_p]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 近义词查找"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSynonyms(key):\n",
    "    a = []\n",
    "    for word in idx_w:\n",
    "        a.append((np.inner(tfidf_w[idx_w[key]], tfidf_w[idx_w[word]]), word))\n",
    "    a.sort(reverse=True)\n",
    "    r = a[0][0]\n",
    "    return [(i/r, j) for i, j in a[:20]]\n",
    "\n",
    "syn = {}\n",
    "for w in idx_w:\n",
    "    syn[w] = getSynonyms(w)\n",
    "\n",
    "syn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关键词组匹配诗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "\n",
    "# a = []\n",
    "# for poem in poems:\n",
    "#     s = set()\n",
    "#     for l in poem[2:]:\n",
    "#         for i in range(1, 4):\n",
    "#             for j in range(len(l)-i):\n",
    "#                 if l[j:j+i] in tfidf_w:\n",
    "#                     s.add(l[j:j+i])\n",
    "#     G = nx.DiGraph()\n",
    "#     for k in keywords:\n",
    "#         G.add_edge('s', k+'k', capacity=1, weight=0)\n",
    "#     for w in s:\n",
    "#         G.add_edge(w, 't', capacity=1, weight=0)\n",
    "#     for k in keywords:\n",
    "#         for w in s:\n",
    "#             G.add_edge(k+'k', w, capacity=1, weight=-int(np.inner(tfidf_w[k], tfidf_w[w])*10000))\n",
    "#     flow = nx.max_flow_min_cost(G, 's', 't')\n",
    "#     a.append((nx.cost_of_flow(G, flow), poem))\n",
    "\n",
    "# a.sort()\n",
    "# for p in a[:5]:\n",
    "#     print(p)\n",
    "\n",
    "def _filterPoems_tfidf(keywords, poems):\n",
    "    if keywords==[]:\n",
    "        return poems\n",
    "    vec = np.zeros((len(idx_w),), dtype=np.float64)\n",
    "    for word in idx_w:\n",
    "        for k in keywords:\n",
    "            v1, v2 = tfidf_w[idx_w[k]], tfidf_w[idx_w[word]]\n",
    "            cos = np.inner(v1, v2)/np.linalg.norm(v1)/np.linalg.norm(v2)\n",
    "            cos = min(1, max(cos, -1))\n",
    "            vec[idx_w[word]] += np.power(np.pi-np.arccos(cos), 5)\n",
    "    vec /= np.linalg.norm(vec)\n",
    "    vec = model_p.transform([vec])[0]\n",
    "    a = []\n",
    "    for idx_p, p in enumerate(poems):\n",
    "        a.append((np.inner(tfidf_p[idx_p], vec), p))\n",
    "    a.sort(key=lambda x: -x[0])\n",
    "    return [x[1] for x in a[:20]], keywords\n",
    "\n",
    "def _filterPoems_2(keywords, poems):\n",
    "    wordWeight = {}\n",
    "    for k in keywords:\n",
    "        for w, word in syn[k]:\n",
    "            wordWeight[word] = wordWeight.get(word, 0)+w\n",
    "    a = []\n",
    "    for p in poems:\n",
    "        w = 0\n",
    "        for l in p['content']:\n",
    "            for i in range(1, 4):\n",
    "                for j in range(len(l)-i):\n",
    "                    if l[j:j+i] in wordWeight:\n",
    "                        w += wordWeight[l[j:j+i]]\n",
    "        a.append((w, p))\n",
    "    a.sort(key=lambda x: -x[0])\n",
    "    return a[:20], list(wordWeight)\n",
    "\n",
    "def _filterPoems(keywords, poems):\n",
    "    if keywords==[]:\n",
    "        return poems, []\n",
    "    return _filterPoems_2(keywords, poems)\n",
    "\n",
    "def filterPoems(x):\n",
    "    x = x.strip().split(' ')\n",
    "    if x[0] in poems:\n",
    "        return _filterPoems(x[1:], poems[x[0]])\n",
    "    else:\n",
    "        return _filterPoems(x, poems['all'])\n",
    "\n",
    "filterPoems(' '.join(['柳', '春', '绿']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez('tfidf', tfidf_p=tfidf_p, tfidf_w=tfidf_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('tfidf', tfidf_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25602deecbb9f532a6e83a36cb0ec82dbee5e0420bae8a06a66cfa16277a7881"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
